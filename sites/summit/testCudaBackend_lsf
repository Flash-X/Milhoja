#!/bin/bash

#BSUB -P AST136
#BSUB -W 0:01
#BSUB -nnodes 1
#BSUB -alloc_flags smt1
#BSUB -J testCudaBackend
#BSUB -o testCudaBackend.%J
#BSUB -e testCudaBackend.%J
 
######################################################################
#####-----              FULLY SPECIFY TEST RUN              -----#####
######################################################################
# IMPORTANT: Match this to X in smtX BSUB statment above
N_HYPERTHREADS_PER_CORE=1

# The StreamManager needs to instruct each thread to sleep for a given amount
# of time so that the duration of the kernel executions is long enough that the
# correct overlaying of computation and communication (enabled by streams) can
# be seen in the nvprof output files.  This time is sufficient for Summit.
SLEEP_TIME_NS=80000

######################################################################
#####-----             DO NOT ALTER LINES BELOW             -----#####
######################################################################
# This should match the SW stack used to build the test binaries
# as specified in buildTestCudaBackend.sh
module purge
module load pgi/19.9
module load spectrum-mpi/10.3.1.2-20200121
module load cuda/10.1.243
module list

# Resource Set = One MPI process/socket + one core per MPI process + 1 GPU per
# MPI process NOTE: We only want one MPI rank in total.  Leave second socket
# idle.
N_PROC_PER_RS=1
N_GPU_PER_PROC=1
N_CORES_PER_PROC=1
N_RS_PER_SOCKET=1
N_RS_PER_NODE=1

# Fixed hardware information
N_SOCKET_PER_NODE=2
N_GPU_PER_SOCKET=3

# Dynamically determine the number of nodes from LSF env var
# The host output contains the word batch1 and repeats of the hosts names
N_HOST_LINES=$(echo $LSB_HOSTS | tr ' ' '\n' | sort -u | wc -l)
N_NODES=$(($N_HOST_LINES - 1))

# Derived results
N_TOTAL_RS=$(($N_NODES * $N_RS_PER_NODE))
N_GPU_PER_RS=$(($N_GPU_PER_PROC * $N_PROC_PER_RS))
N_CORES_PER_RS=$(($N_CORES_PER_PROC * $N_PROC_PER_RS))
N_THREADS_PER_PROC=$(($N_HYPERTHREADS_PER_CORE * $N_CORES_PER_PROC))

# No need for OpenMP
export OMP_NUM_THREADS=1

echo "---------------------------------------------------------------------"
echo "Number of Nodes               = $N_NODES"
echo "Number of RS Per Node         = $N_RS_PER_NODE"
echo "Number of Resource Sets (RS)  = $N_TOTAL_RS"
echo "Number of Processes per RS    = $N_PROC_PER_RS"
echo "Number of GPU per Process     = $N_GPU_PER_PROC"
echo "Number of Cores per Process   = $N_CORES_PER_PROC"
echo "Number of HW Threads per Core = $N_HYPERTHREADS_PER_CORE"
echo "Number of Threads per Process = $N_THREADS_PER_PROC"
echo "---------------------------------------------------------------------"

folder=/gpfs/alpine/scratch/joneal/ast136/OrchestrationRuntime/testCudaBackend_$LSB_JOBID
mkdir -p $folder ; cd $folder

jsrun -n $N_RS_PER_NODE \
      -r $N_RS_PER_NODE \
      -a $N_PROC_PER_RS \
      -c $N_CORES_PER_RS \
      -g $N_GPU_PER_RS \
      --smpiargs="-disable_gpu_hooks" \
      nvprof -o TestCudaBackend.%h.%p $LS_SUBCWD/binaries/test_cuda_backend.x $SLEEP_TIME_NS

jsrun -n $N_RS_PER_NODE \
      -r $N_RS_PER_NODE \
      -a $N_PROC_PER_RS \
      -c $N_CORES_PER_RS \
      -g $N_GPU_PER_RS \
      --smpiargs="-disable_gpu_hooks" \
      nvprof -o TestCudaBackend_debug.%h.%p $LS_SUBCWD/binaries/test_cuda_backend_debug.x $SLEEP_TIME_NS

