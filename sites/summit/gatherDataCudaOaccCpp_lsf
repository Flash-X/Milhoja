#!/bin/bash

#BSUB -P AST136
#BSUB -W 0:15
#BSUB -nnodes 1
#BSUB -alloc_flags smt1
#BSUB -J gatherDataCudaOaccCpp
#BSUB -o gatherDataCudaOaccCpp.%J
#BSUB -e gatherDataCudaOaccCpp.%J
 
######################################################################
#####-----              FULLY SPECIFY TEST RUN              -----#####
######################################################################
# IMPORTANT: Match this to X in smtX BSUB statment above
N_HYPERTHREADS_PER_CORE=1

######################################################################
#####-----             DO NOT ALTER LINES BELOW             -----#####
######################################################################
# This should match the SW stack used to build the test binaries
# as specified in buildGatherDataCpp.sh
module purge
module load pgi/19.9
module load spectrum-mpi/10.3.1.2-20200121
module load cuda/10.1.243
module list

# Resource Set = One MPI process/socket + maximal number of cores per MPI process
# NOTE: We only want one MPI rank in total.  Leave second socket idle.
N_PROC_PER_RS=1
N_GPU_PER_PROC=1
N_CORES_PER_PROC=7
N_RS_PER_SOCKET=1
N_RS_PER_NODE=1

# Fixed hardware information
N_SOCKET_PER_NODE=2
N_GPU_PER_SOCKET=3

# Dynamically determine the number of nodes from LSF env var
# The host output contains the word batch1 and repeats of the hosts names
N_HOST_LINES=$(echo $LSB_HOSTS | tr ' ' '\n' | sort -u | wc -l)
N_NODES=$(($N_HOST_LINES - 1))

# Derived results
N_TOTAL_RS=$(($N_NODES * $N_RS_PER_NODE))
N_GPU_PER_RS=$(($N_GPU_PER_PROC * $N_PROC_PER_RS))
N_CORES_PER_RS=$(($N_CORES_PER_PROC * $N_PROC_PER_RS))
N_THREADS_PER_PROC=$(($N_HYPERTHREADS_PER_CORE * $N_CORES_PER_PROC))

# No need for OpenMP
export OMP_NUM_THREADS=1

echo "---------------------------------------------------------------------"
echo "Number of Nodes               = $N_NODES"
echo "Number of RS Per Node         = $N_RS_PER_NODE"
echo "Number of Resource Sets (RS)  = $N_TOTAL_RS"
echo "Number of Processes per RS    = $N_PROC_PER_RS"
echo "Number of GPU per Process     = $N_GPU_PER_PROC"
echo "Number of Cores per Process   = $N_CORES_PER_PROC"
echo "Number of HW Threads per Core = $N_HYPERTHREADS_PER_CORE"
echo "Number of Threads per Process = $N_THREADS_PER_PROC"
echo "---------------------------------------------------------------------"

folder=/gpfs/alpine/scratch/joneal/ast136/OrchestrationRuntime/gatherDataCudaOaccCpp_$LSB_JOBID
mkdir -p $folder ; cd $folder

date
for binary in $LS_SUBCWD/binaries/gather_data_cuda_oacc_cpp_*_*.x; do
    time jsrun -n $N_RS_PER_NODE \
               -r $N_RS_PER_NODE \
               -a $N_PROC_PER_RS \
               -c $N_CORES_PER_RS \
               -g $N_GPU_PER_RS \
               -l CPU-CPU -d cyclic -b rs \
               $binary
done
date

